kind: ServiceAccount
apiVersion: v1
metadata:
  name: tigera-es-proxy
  namespace: calico-monitoring
---

# Service for access to elasticsearch from EE Manager, backed by the ES Proxy.
apiVersion: v1
kind: Service
metadata:
  name: elasticsearch-tigera-elasticsearch
  namespace: calico-monitoring
spec:
  selector:
    app: es-proxy
  type: ClusterIP
  ports:
  - port: 8443
    targetPort: 8443

---

apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: tigera-es-proxy
  namespace: calico-monitoring
  labels:
    app: es-proxy
spec:
  replicas: 1
  strategy:
    type: Recreate
  template:
    metadata:
      name: tigera-es-proxy
      namespace: calico-monitoring
      labels:
        app: es-proxy
      annotations:
        # Mark this pod as a critical add-on; when enabled, the critical add-on scheduler
        # reserves resources for critical add-on pods so that they can be rescheduled after
        # a failure.  This annotation works in tandem with the toleration below.
        scheduler.alpha.kubernetes.io/critical-pod: ''
    spec:
      nodeSelector:
        beta.kubernetes.io/os: linux
      serviceAccountName: tigera-es-proxy
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      # Allow this pod to be rescheduled while the node is in "critical add-ons only" mode.
      # This, along with the annotation above marks this pod as a critical add-on.
      - key: CriticalAddonsOnly
        operator: Exists
      imagePullSecrets:
        - name: cnx-pull-secret
      # ex-proxy-init-pem writes a "frontend.pem" file into a volume mounted in the tigera-es-proxy container.
      initContainers:
      - name: es-proxy-init-pem # create pem for tigera-es-proxy
        image: alpine:3.7
        command: ['sh', '-c', 'cat /etc/es-proxy-tls/frontend.crt /etc/es-proxy-tls/frontend.key > /etc/es-proxy-tls-cache/frontend.pem']
        volumeMounts:
        - mountPath: /etc/es-proxy-tls
          name: es-proxy-tls
        - mountPath: /etc/es-proxy-tls-cache
          name: es-proxy-tls-cache
      containers:
      - name: tigera-es-proxy
        image: {{page.registry}}{{site.imageNames["es-proxy"]}}:{{site.data.versions[page.version].first.components["es-proxy"].version}}
        env:
          - name: ES_BACKEND_HOST
            valueFrom:
              configMapKeyRef:
                name: tigera-es-proxy
                key: elasticsearch.backend.host
          - name: ES_BACKEND_PORT
            valueFrom:
              configMapKeyRef:
                name: tigera-es-proxy
                key: elasticsearch.backend.port
          - name: ES_BACKEND_BASIC_AUTH_TOKEN
            valueFrom:
              secretKeyRef:
                name: tigera-es-proxy
                key: backend.authHeader
          - name: ELASTIC_INDEX_SUFFIX
            valueFrom:
              configMapKeyRef:
                name: tigera-es-config
                key: tigera.elasticsearch.cluster-name
        livenessProbe:
          exec:
            command:
              - sh
              - -c
              - /bin/liveness.sh
          initialDelaySeconds: 90
          periodSeconds: 10
        volumeMounts:
        - mountPath: /etc/es-proxy-tls
          name: es-proxy-tls
        - mountPath: /etc/es-proxy-tls-cache
          name: es-proxy-tls-cache
      volumes:
      - name: es-proxy-tls-cache
        emptyDir: {}
      - name: es-proxy-tls
        secret:
          secretName: tigera-es-proxy
---

# Allow access to es-proxy from kube-apiserver
apiVersion: projectcalico.org/v3
kind: GlobalNetworkPolicy
metadata:
  name: allow-cnx.es-proxy-access
spec:
  order: 1
  tier: allow-cnx
  selector: app == 'es-proxy'
  types:
  - Ingress
  ingress:
  - action: Allow
    protocol: TCP
    source:
      selector: role == 'k8s-apiserver-endpoints'
    destination:
      ports: [8443]
